{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc6b8732-7257-4be2-85b0-f51438a3a2c4",
   "metadata": {
    "tags": [
     "problem"
    ]
   },
   "source": [
    "# Survivor Prediction on Titanic using Machine Learning\n",
    "\n",
    "## Objective: Predict survival on the Titanic\n",
    "\n",
    "## Description\n",
    "\n",
    "Visit [Titanic - Machine Learning from Disaster Competition on Kaggle](https://www.kaggle.com/competitions/titanic/overview) to view the challenge description and instructions.\n",
    "\n",
    "## My Approach\n",
    "\n",
    "Here is a brief description of my approach. I have included detailed explanation with the code cells.\n",
    "\n",
    "My first submission score was 0.6866 using K-Nearest Neighbour technique and I trained the model with only 3 features - `Pclass`, `Sex` and `Age`.\n",
    "\n",
    "For my next 7 submissions, I did hyperparameter tuning using `sklearn.mode_selection.GridSearchCV()` method and included five features - `Pclass`, `Sex`, `Age`, `SibSp` and `Parch`. I used several techniques including K-Nearest Neighbour, Logistic Regression, Decision Tree to train and predict. The train and test accuracy on labelled data were between 0.73 and 0.81. My submission scores were between 0.72 and 0.77.\n",
    "\n",
    "For my next submissions, I changed the approach for addressing missing values in `Age` column and also implemented **Feature Engineering** wherein new features were added.\n",
    "\n",
    "- The calculation of missing ages is described with the code cells below.  \n",
    "[Jump to Handling missing values](#Handling-missing-values).\n",
    "- From `Parch` and `SibSp`, I derived two columns `FamilySize` and `IsAlone`. `FamilySize` contain numerical values and `IsAlone` contain boolean values.  \n",
    "[Jump to Interaction Feature](#Interaction-Feature-using-SibSp-and-Parch).\n",
    "- From `FamilySize` feature created above, I created `FamilySizeGroup` feature to divide the family size in categorical values - `Single`, `Small` and `Large` and later on assigned numerical values to these categories using `preprocessing.LabelEncoder()` method.  \n",
    "[Jump to Family Size Bins](#Family-Size-Bins).\n",
    "- From `Age` column, I created bins of Ages using `pandas.qcut()` method such that the distribution of passengers among bins are equal. The bins were provided numerical values which are dependent categorical values.  \n",
    "[Jump to Age Bins](#Age-Bins).\n",
    "- From `Fare` column, I created bins of Fares using `pandas.qcut()` method such that the distribution of passengers among bins are equal. The bins were provided numerical values which are dependent categorical values.  \n",
    "[Jump to Fare Bins](#Fare-Bins).\n",
    "- On `Sex` column, I applied **One-hot encoding** for the categories which created two additional columns containing boolean values. I dropped `Sex` column from my training and test data.  \n",
    "[Jump to Encode Categorical Data](#Encode-Categorical-Data).\n",
    "- From `Name` column, I extracted Title information and then applied **One-hot encoding** to the title categories. It added 5 new columns.  \n",
    "[Jump to Titles](#Titles)  \n",
    "[Jump to Encode Categorical Data](#Encode-Categorical-Data).\n",
    "- I also created new features using `Ticket` column, but then the model got overly complex and it reduced my submission score to 0.72996. I dropped these new features along with `Ticket` from the dataset.\n",
    "\n",
    "\n",
    "Application of the above methods results in submission scores > 0.77.\n",
    "\n",
    "**MY LATEST SUBMISSION SCORE IS 0.78708. I plan to break the 0.8 barrier.**\n",
    "\n",
    "---\n",
    "\n",
    "**I have included several ML techniques in the code. The tuning and training is performed with all the techniques and the predictions from each techniques are stored in the csv files separately. The code will also generate the confusion matrix and provide the accuracy score on training data for each ML algorithm.**  \n",
    ">[Jump to Models Development](#Model-Development-using-Training-Data).\n",
    "\n",
    "\n",
    "**Please feel free to share your feedback/suggestions/appreciation on my work.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a88706-5dc4-4d01-be8a-d695a18a592c",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53af739e-7eff-43d3-838b-f64be301594b",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc5f878-6f6c-41f2-917f-879cd4e0c4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import sklearn.tree as tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd47177-b618-4cfb-9011-5c3c0c097511",
   "metadata": {},
   "source": [
    "### Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdf72f6-01c8-481e-8037-10facef72008",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"./kaggle-titanic-comp/train.csv\")\n",
    "df_train.set_index('PassengerId', inplace=True)\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28babab-1d45-4a93-a862-981362e852d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e35aa17-310a-4a93-82a4-4eaa2aa6958c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17ff747-1e77-4284-adae-cb9dce6967e2",
   "metadata": {},
   "source": [
    "From the dataset snapshot and info, we can make some initial observations to understand our data and to determine the preprocessing required in our dataset.\n",
    "- `Name`, `Sex`, `Ticket`, `Cabin`, and `Embarked` columns contain string datatype which we will need to convert to numeric datatype.\n",
    "- Some of the values in `Age` and `Cabin` columns are NaN. We need to address these null type values if we have to keep these columns in our analysis.\n",
    "- `Sex` columns contain categorical values which can be easily converted to numeric datatype.\n",
    "- `Name` column contains title that we can extract and make it useful in our analysis\n",
    "- We can use the counts in `SibSp` and `Parch` columns to identify the number of family members traveling with the passenger.\n",
    "\n",
    "Let's dive deeper into our dataset and find out which columns we need in our analysis and the feature engineering that we have to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f742fc0c-6b3f-42dc-88a9-20ebf00ec539",
   "metadata": {
    "tags": [
     "info"
    ]
   },
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323df7bf-4086-4548-9fc3-f940288fec53",
   "metadata": {
    "tags": [
     "preprocess"
    ]
   },
   "source": [
    "### Handling missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fb7ee1-8735-4913-aedd-977ab5f81626",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Find out which columns has missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4af920-95ab-4c17-ab27-56e4f98b22e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0783fa79-c85b-4342-91fb-0d6e0d2c1972",
   "metadata": {},
   "source": [
    "So we have 177 values missing in the `Age` column, 687 in the `Cabin` column and 2 in the `Embarked` column. Let's address each of these columns individually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b661a558-b644-4da3-8c4b-9f805966bedb",
   "metadata": {},
   "source": [
    "#### Missing Values in Age Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a72aa9-f0f5-4c8c-be47-d59d092521f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Age'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cf9f4d-5481-4b3a-a58d-f648822aef31",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Histogram of passengers in different age groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05adcfc-ceb2-481c-96ab-2dcafa7ea17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(df_train['Age'].dropna(), bins=20)\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Histogram of Age')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb7283d-11a8-4e31-9f65-dbc2f5913cb5",
   "metadata": {},
   "source": [
    "From the above histogram plot we can see that most passengers were aged between 20 and 40 years of age and median age of the dataset is 28 years."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae89eca-cc0e-497d-8ea8-de4ba7eb5f55",
   "metadata": {},
   "source": [
    "Usually, median is the best approach to address the missing age values. However, if we are inserting the median age we will be putting all those passengers with different features in the same age catggory. Instead, we must take into consideration other features of the passengers (with missing ages) before assigning any age values to them.\n",
    "\n",
    "We know that the travelling passengers may belong to different age categories based on their sex, passenger class, embarked port, sibling/spouse count, and parent/child count. So let's first explore each of these features indivually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c96b91-5f13-43e1-a237-3c57b2e689b7",
   "metadata": {},
   "source": [
    "##### Histogram of males and females passenger in different age groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83b4424-245b-4542-ba5b-419b6223baaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "men = df_train[df_train[\"Sex\"] == \"male\"]\n",
    "women = df_train[df_train[\"Sex\"] == \"female\"]\n",
    "\n",
    "# create a histogram of age for men\n",
    "plt.hist(men[\"Age\"].dropna(), bins=30, alpha=0.5, label=\"Men\")\n",
    "\n",
    "# create a histogram of age for women\n",
    "plt.hist(women[\"Age\"].dropna(), bins=30, alpha=0.5, label=\"Women\")\n",
    "\n",
    "# add labels and title to the histogram\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Age Distribution for Men and Women\")\n",
    "plt.legend()\n",
    "\n",
    "# display the histogram\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940a2a42-0734-411e-a94c-c44639a33b50",
   "metadata": {},
   "source": [
    "##### Check the count of missing age values in different column features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12928ab9-46b1-4b9c-8faf-7b8c772a9d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_na = df_train[df_train['Age'].isna()]\n",
    "#df_na['Embarked'].value_counts()\n",
    "print(\"Bifurcation of missing age values in the following column features\")\n",
    "print(f\"in Sex:\\n{df_na['Sex'].value_counts()}\")\n",
    "print(f\"in Pclass:\\n{df_na['Pclass'].value_counts()}\")\n",
    "print(f\"in Embarked:\\n{df_na['Embarked'].value_counts()}\")\n",
    "print(f\"in SibSp:\\n{df_na['SibSp'].value_counts()}\")\n",
    "print(f\"in Parch:\\n{df_na['Parch'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324f602e-fea4-48d8-ba46-da98dc3fc7ac",
   "metadata": {},
   "source": [
    "##### Find the median age of each of these categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df5cd26-661d-468f-b14e-ec83b462c2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.groupby(['Pclass'])['Age'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a4054b-32c5-4e7a-8320-3ec587ad0820",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.groupby(['Sex'])['Age'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575ee6d5-1531-41e2-b3bc-7ea9fe0bbd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.groupby(['Embarked'])['Age'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0337c044-02ef-4db3-99f3-a117fc4063be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.groupby(['SibSp'])['Age'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce78998-e096-4478-a0af-f72bf9f7ba2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.groupby(['Parch'])['Age'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a284334-cd35-4120-a182-2f91b0fe2fea",
   "metadata": {},
   "source": [
    "**Take a close look at the above median age values in different features and categories. These median values gives us a fair idea that filling the missing Age values with the median age of the whole dataset is not the right approach and it might lead to inaccuracies in prediction. Instead I am going to fill the missing Age values with a median age of the passenger's combined categories.**\n",
    "\n",
    "For example, find the median age of males, traveling in Pclass = 3, Embarked at S, Have no Sibling/Spouse, and Have no Parent/Child and fill the missing age of the passengers that falls in the above category. Similarly, I will find the median age of all the combinations and fill the missing age values using loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b80861-96fc-4933-b693-3742e9dc52e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This next code line is only an example. Full operation is performed in the next code cell.\n",
    "# Find the median age of males, traveling in Pclass = 3, Embarked at S, Have no Sibling/Spouse, and Have no Parent/Child\n",
    "df_train[(df_train['Sex']=='male') & (df_train['Pclass']==3) & (df_train['Embarked']=='S') & (df_train['SibSp']==0) & (df_train['Parch']==0)]['Age'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1f6f58-bd31-48bb-8088-7f7d9f835db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sex=['male', 'female']\n",
    "pclasses = [1, 2, 3]\n",
    "embarked = ['C', 'Q', 'S']\n",
    "sibsp = [0, 1, 2, 3, 8]\n",
    "parch = [0, 1, 2]\n",
    "\n",
    "median_ages = []\n",
    "for sx in sex:\n",
    "    for pc in pclasses:\n",
    "        for e in embarked:\n",
    "            for s in sibsp:\n",
    "                for p in parch:\n",
    "                    mask = (df_train['Sex'] == sx) & (df_train['Pclass'] == pc) & (df_train['Embarked'] == e) & (df_train['SibSp'] == s) & (df_train['Parch'] == p)\n",
    "                    median_age = df_train.loc[mask, 'Age'].median()\n",
    "                    df_train.loc[mask & (df_train['Age'].isna()), 'Age'] = median_age"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a79f0a-5c9a-4b2b-8305-09247182ccdc",
   "metadata": {},
   "source": [
    "##### Check the count of none values in thr `Age` feature again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609b5708-be2e-43e0-b47b-e497bdd3e231",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Age'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34b4a6c-2952-40be-ae76-f2f78da36dab",
   "metadata": {},
   "source": [
    "So, I have still 20 none type values in the Age column. Finding these values requires further research about the passenger features whose age is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10be8f8-608e-4360-b117-c25eb0a8b09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_na = df_train[df_train['Age'].isna()]\n",
    "df_na"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef95490-4381-4006-995e-fc7945ad9376",
   "metadata": {},
   "source": [
    "These age values were not filled because there were no other people matching the features of the above passengers, and thus no median value exist.  \n",
    "\n",
    "Let's fill out the ages of these passengers individually. Take a look at the above data and you will notice the following points:\n",
    "\n",
    "1. `PassengerId = 47, 110, 187, 215, 242, 365, 613, 769` have the same features, i.e. travelled in `Pclass=3`, `SibSp=1`, `Parch=0`, except that they are either male or female and embarked at different ports.\n",
    "2. `PassengerId = 49, 302, 331` have the same features, i.e. travelled in `Pclass=3`, `SibSp=2`, `Parch=0`, except that they are either male or female and embarked at different ports.\n",
    "3. `PassengerId = 160, 181, 202, 325, 793, 847, 864` are Sage family travelled in `Pclass=3`, `SibSp=8`, `Parch=2`. We will deal with the Sage family age separately.\n",
    "4. `PassengerId = 594` travelled in `Pclass=3`, `SibSp=0`, `Parch=2`, `Sex=female` and `PassengerId = 889` travelled in `Pclass=3`, `SibSp=1`, `Parch=2`, `Sex=female`. These two passengers can be combined in the same category.\n",
    "\n",
    "Let's analyse the above points one by one to find out the best approach to represent their age."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c65c576-0ade-4628-82ec-c51929edbf17",
   "metadata": {
    "tags": []
   },
   "source": [
    "**1. Find the median age of passengers traveled in `Pclass=3`, `SibSp=1`, `Parch=0` and leave out the `Sex` and `Embarked` features. I have left out `Sex` feature because I have checked that median is same for both genders when it is included in our calculation criteria.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c824a315-813c-485c-a692-1c0c5377c1ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "median = df_train[(df_train['Pclass']==3) & (df_train['SibSp']==1) & (df_train['Parch']==0)]['Age'].median()\n",
    "median"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354ebe83-2754-4788-9cc7-58ecd498991c",
   "metadata": {},
   "source": [
    "Fill the ages of `PassengerId = 47, 110, 187, 215, 242, 365, 613, 769` with the above calculated median age = 25 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3081b86b-d24c-41f2-9ddd-b6f6f05e79c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pid = [47, 110, 187, 215, 242, 365, 613, 769]\n",
    "\n",
    "for p in pid:\n",
    "    df_train.loc[p, 'Age'] = median"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16781b96-83d3-4dd9-9c86-c79702cccb28",
   "metadata": {},
   "source": [
    "**2. Find the median age of passengers traveled in `Pclass=3`, `SibSp=2`, `Parch=0`, `Sex=male/female` and leave out the `Embarked` feature.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ee6d77-338f-4c7b-acff-d2eea11cbc0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "median_male = df_train[(df_train['Sex']=='male') & (df_train['Pclass']==3) & (df_train['SibSp']==2) & (df_train['Parch']==0)]['Age'].median()\n",
    "median_female = df_train[(df_train['Sex']=='female') & (df_train['Pclass']==3) & (df_train['SibSp']==2) & (df_train['Parch']==0)]['Age'].median()\n",
    "print(f\"Male median for the above calculation criteria is: {median_male} and female median is: {median_female}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49132287-e81f-4b3c-b4b1-6c27e546c394",
   "metadata": {},
   "source": [
    "Fill the ages of `PassengerId = 49, 302` with the above calculated male median age = 27 years and `PassengerId = 331` with female median age = 18 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa8fdfe-5d94-4db4-8769-857c5cae70ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train.loc[49, 'Age'] = median_male\n",
    "df_train.loc[302, 'Age'] = median_male\n",
    "df_train.loc[331, 'Age'] = median_female"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15e9bdf-b612-450f-8da2-01c8bec69099",
   "metadata": {},
   "source": [
    "**3. Find the median age of passengers with the feature `Parch=2`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca12f59-9c6f-4bea-a7c0-88747c40670e",
   "metadata": {},
   "outputs": [],
   "source": [
    "median = df_train[(df_train['Parch']==2)]['Age'].median()\n",
    "median"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e91fdf-1f7f-4cb8-989e-83ad998c85d5",
   "metadata": {},
   "source": [
    "So, looking at the titles of the Sage family passengers, I am confident that `PassengerId=160` having title Master was aged under 18 years. I will use the above calculated median age = 15 years for the above passenger. For the rest of the Sage family, I am  not sure about their age group and therefore, for them I will provide the median age of all the passengers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcad015-4554-4667-8735-657fb4b4bf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc[160, 'Age'] = median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edbc863-e088-4de3-8898-9e135f2e553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "median = df_train['Age'].median()\n",
    "median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80180e2a-904b-414b-b170-4a0b13013397",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pid = [181, 202, 325, 793, 847, 864]\n",
    "\n",
    "for p in pid:\n",
    "    df_train.loc[p, 'Age'] = median"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d79d849-3b96-466f-a7e4-43a5e093d2a3",
   "metadata": {},
   "source": [
    "**4. Find the median age of passengers traveled in `Pclass=3`, `SibSp=0`, `Parch=2`, `Sex=female` and leave out the `Embarked` feature.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4105a328-e4e7-4437-b40c-963d81eacb68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "median = df_train[(df_train['Sex']=='female') & (df_train['Pclass']==3) & (df_train['SibSp']==0) & (df_train['Parch']==2)]['Age'].median()\n",
    "median"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7eee61f-e2e6-4d65-900f-9a7147ed30cb",
   "metadata": {},
   "source": [
    "Fill the ages of `PassengerId = 594, 889` with the above calculated median age = 15 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47bcb99-c584-4d48-8ee8-7678880c28e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc[594, 'Age'] = median\n",
    "df_train.loc[889, 'Age'] = median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2316fa12-122e-44be-a0c1-96c727f7adec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b507c2d1-bcec-4b9c-83b7-bfa82e03354d",
   "metadata": {},
   "source": [
    "#### Missing Values in Cabin Column\n",
    "\n",
    "Since the percentage of the missing values in Cabin column is very high. I will drop the Cabin column from my analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b318d18-fdc1-49fc-b234-7849cc12d61d",
   "metadata": {},
   "source": [
    "#### Missing Values in Embarked Column\n",
    "\n",
    "##### Plot Passenger Count vs Embarked Ports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3590405b-601e-4be7-bdbb-27e1ffcd0328",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(df_train['Embarked'].value_counts().index, df_train['Embarked'].value_counts(), width=0.4, alpha=0.8)\n",
    "plt.xlabel(\"Embarked Ports\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Embarked Count\")\n",
    "plt.show()\n",
    "df_train['Embarked'].mode()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bb641f-98b5-47d3-930e-aa396a9d9bfb",
   "metadata": {},
   "source": [
    "Most of the passengers embarked at 'S' port. And since we have only 2 missing port value, I will fill those missing values with the mode value of the `Embarked` feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ad75ee-c7ef-42a6-b57c-4dadbea0085d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Embarked'].fillna(df_train['Embarked'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662de83f-9c2c-4045-b4f3-1acc72a2c011",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a3695a-f0d5-40e1-83a5-c394a8a8b4cb",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis\n",
    "\n",
    "#### Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac807c9-775d-454c-bed0-79cbfa800cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(['Cabin'], axis=1, inplace=True)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ac7a7f-07e4-4516-b9a2-85479bf8cd89",
   "metadata": {},
   "source": [
    "##### Survived and deaths count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51a9555-1cb7-4691-8a33-70426c0767ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Survived'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a658cc4c-03fd-4539-a6e5-42879a860ce7",
   "metadata": {},
   "source": [
    "##### Gender\n",
    "We can see in the pivot table that females has more chance of the survival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa15df1f-f92f-441d-ab0e-02ec3de1721f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.pivot_table(values='Survived', index='Sex', columns=df_train['Survived'], aggfunc='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23988b24-bf2b-4164-9e0f-5eb0c1cb0494",
   "metadata": {},
   "source": [
    "##### Social Economix Status\n",
    "\n",
    "From the below plot, we can tell that a Class 1 passenger has more chance of surviving than a Class 3 passenger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73566ade-053d-4c68-ba63-d6022c22b673",
   "metadata": {},
   "outputs": [],
   "source": [
    "psgr_survived = df_train[df_train['Survived']==1]\n",
    "\n",
    "plt.bar(df_train['Pclass'].value_counts().index, df_train['Pclass'].value_counts(), width=0.4, color='red', alpha=0.5, label='Total Passengers')\n",
    "plt.bar(psgr_survived['Pclass'].value_counts().index, psgr_survived['Pclass'].value_counts(), width=0.4, color='blue', alpha=0.7, label='Passengers Survived')\n",
    "plt.xticks([1, 2, 3])\n",
    "plt.legend()\n",
    "plt.xlabel(\"Passenger Class\")\n",
    "plt.ylabel(\"Passenger Count\")\n",
    "plt.title(\"Survival Likelihood based on Socio-Economic Status\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a12908-dd80-4686-abf1-f9bacc9b4a3a",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Age\n",
    "\n",
    "Passengers below 8 years and above 35 years of age had a better chance of surviving as compared to passengers aged between 15 and 35 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e7ae81-e35b-4bc2-9f41-2b7c78b3a358",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_train['Age'], color='red', bins=20, alpha=0.5, label='Total Passengers')\n",
    "plt.hist(psgr_survived['Age'], color='blue', bins=20, alpha=0.7, label='Passengers Survived')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Passenger Age\")\n",
    "plt.ylabel(\"Passenger Count\")\n",
    "plt.title(\"Survival Likelihood based on Age\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4e2016-ab55-47c5-8f8e-74aab362568d",
   "metadata": {},
   "source": [
    "##### Embarkment Port\n",
    "\n",
    "The passengers embarked at port 'S' has the worst ratio of survived passenger. However, looking at the numbers below, the Embarkment port feature doesn't provide a strong conclusion about the chances of survival, thus I will drop this feature from my analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb512a5-5dd8-4dbf-8fba-6a89eee42f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(df_train['Embarked'].value_counts().index, df_train['Embarked'].value_counts(), width=0.4, color='red', alpha=0.5, label='Total Passengers')\n",
    "plt.bar(psgr_survived['Embarked'].value_counts().index, psgr_survived['Embarked'].value_counts(), width=0.4, color='blue', alpha=0.7, label='Passengers Survived')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Port of Embarkment\")\n",
    "plt.ylabel(\"Passenger Count\")\n",
    "plt.title(\"Survival Likelihood based on Embarkment Port\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bea6bf-67f5-4c0e-9cf6-e5c03ac4ee08",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Survival percentage for passengers embarked at different ports\\\n",
    ":\\n{round(psgr_survived['Embarked'].value_counts()/df_train['Embarked'].value_counts()*100,3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2735bcd-c57e-42a0-8315-b5a02571230c",
   "metadata": {},
   "source": [
    "##### Sibling/Spouse and Parent/Child\n",
    "\n",
    "Most passengers were travelling without parents or children.  \n",
    "Most passengers were travelling without siblings or a spouse.  \n",
    "The data also suggests that the passengers who were traveling alone have a lower survival chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f873222c-c379-4e4a-bfa0-ec81986865f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "psgr_survived = df_train[df_train['Survived']==1]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax[0].bar(df_train['SibSp'].value_counts().index, df_train['SibSp'].value_counts(), width=0.4, color='red', alpha=0.5, label='Total Passengers')\n",
    "ax[0].bar(psgr_survived['SibSp'].value_counts().index, psgr_survived['SibSp'].value_counts(), width=0.4, color='blue', alpha=0.7, label='Passengers Survived')\n",
    "#plt.xticks([1, 2, 3])\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel(\"Sibling/Spouse\")\n",
    "ax[0].set_ylabel(\"Passenger Count\")\n",
    "ax[0].set_ylim(0, 715)\n",
    "ax[0].set_title(\"Survival Likelihood based on\\nSibling Spouse count\")\n",
    "\n",
    "ax[1].bar(df_train['Parch'].value_counts().index, df_train['Parch'].value_counts(), width=0.4, color='red', alpha=0.5, label='Total Passengers')\n",
    "ax[1].bar(psgr_survived['Parch'].value_counts().index, psgr_survived['Parch'].value_counts(), width=0.4, color='blue', alpha=0.7, label='Passengers Survived')\n",
    "#plt.xticks([1, 2, 3])\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel(\"Parent/Child\")\n",
    "ax[1].set_ylabel(\"Passenger Count\")\n",
    "ax[1].set_title(\"Survival Likelihood based on\\nParent Child count\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a18f6f-4efa-4698-bf33-23edb7a84c08",
   "metadata": {},
   "source": [
    "#### Multivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e50755-586e-4a31-ae06-079b9a00cb07",
   "metadata": {},
   "source": [
    "##### Sex and Socio-Economic Status\n",
    "\n",
    "Most of the Class 1 or Class 2 female passengers survived as compared to Class 3 female passengers. Among males, the likelihood of survival are more if the passenger belonged to Class 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e08f64-bac7-462a-a34a-c49e3a77d412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the data to include only males and females\n",
    "ml_all = df_train[df_train['Sex'] == 'male']['Pclass'].value_counts()\n",
    "ml_srv = df_train[(df_train['Sex'] == 'male') & (df_train['Survived'] == 1)]['Pclass'].value_counts()\n",
    "fm_all = df_train[df_train['Sex'] == 'female']['Pclass'].value_counts()\n",
    "fm_srv = df_train[(df_train['Sex'] == 'female') & (df_train['Survived'] == 1)]['Pclass'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9b8e33-3ec4-481a-a07e-e6f12c2196b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the figure and axis objects\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax[0].bar(ml_all.index, ml_all, width=0.4, color='blue', alpha=0.4, label='Total Males')\n",
    "ax[0].bar(ml_srv.index, ml_srv, width=0.4, color='green', alpha=0.6, label='Males Survived')\n",
    "ax[0].set_title('Male Survival Likelihood\\nbased on Socio-economic Status')\n",
    "ax[0].set_xlabel(\"Passenger Class\")\n",
    "ax[0].set_ylabel(\"Passenger Count\")\n",
    "ax[0].set_xticks([1, 2, 3])\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].bar(fm_all.index, fm_all, width=0.4, color='pink', alpha=0.8, label='Total Females')\n",
    "ax[1].bar(fm_srv.index, fm_srv, width=0.4, color='green', alpha=0.6, label='Females Survived')\n",
    "ax[1].set_title('Female Survival Likelihood\\nbased on Socio-economic Status')\n",
    "ax[1].set_xlabel(\"Passenger Class\")\n",
    "#ax[1].set_ylabel(\"Passenger Count\")\n",
    "ax[1].set_xticks([1, 2, 3])\n",
    "ax[1].legend()\n",
    "\n",
    "plt.ylim(0, 360)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5acf80-f9ce-4244-bd41-b1555c6aa751",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.DataFrame({'Sex': ['Males', 'Females'],\n",
    "                     'Class 1': [round(ml_srv[1]/ml_all[1]*100, 3), round(fm_srv[1]/fm_all[1]*100, 3)],\n",
    "                     'Class 2': [round(ml_srv[2]/ml_all[2]*100, 3), round(fm_srv[2]/fm_all[2]*100, 3)],\n",
    "                     'Class 3': [round(ml_srv[3]/ml_all[3]*100, 3), round(fm_srv[3]/fm_all[3]*100, 3)]\n",
    "                     })\n",
    "table.set_index(['Sex'], inplace=True)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca2f0c8-18a8-445f-b1ea-7a82bdd2c524",
   "metadata": {},
   "source": [
    "##### Sex and Age\n",
    "\n",
    "Higher percentage of females aged between 0 to 8 years and 35 to 60 years survived the disaster, whereas, for males the survival percent is higher for ages between 0 to 10 years and 25 to 35 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53529a79-00f8-4fed-a422-f8bbdcd34df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the data to include only males and females\n",
    "ml_all = df_train[df_train['Sex'] == 'male']['Age']\n",
    "ml_srv = df_train[(df_train['Sex'] == 'male') & (df_train['Survived'] == 1)]['Age']\n",
    "fm_all = df_train[df_train['Sex'] == 'female']['Age']\n",
    "fm_srv = df_train[(df_train['Sex'] == 'female') & (df_train['Survived'] == 1)]['Age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917349cd-3ebf-4840-9b50-007d3be17ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the figure and axis objects\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax[0].hist(ml_all, color='blue', alpha=0.4, label='Total Males')\n",
    "ax[0].hist(ml_srv, color='green', alpha=0.6, label='Males Survived')\n",
    "ax[0].set_title('Male Survival Likelihood\\nbased on Age')\n",
    "ax[0].set_xlabel(\"Passenger Age\")\n",
    "ax[0].set_ylabel(\"Passenger Count\")\n",
    "ax[0].set_xticks(np.linspace(0, 80, 9))\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].hist(fm_all, color='pink', alpha=0.8, label='Total Females')\n",
    "ax[1].hist(fm_srv, color='green', alpha=0.6, label='Females Survived')\n",
    "ax[1].set_title('Female Survival Likelihood\\nbased on Age')\n",
    "ax[1].set_xlabel(\"Passenger Age\")\n",
    "ax[1].set_xticks(np.linspace(0, 80, 9))\n",
    "ax[1].legend()\n",
    "\n",
    "plt.xlim(0, 85)\n",
    "plt.ylim(0, 220)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2789424-e3b8-4cda-bb25-f1d41bfb17cb",
   "metadata": {},
   "source": [
    "##### Sex, Age and Sibling/Spouse\n",
    "Most passengers were travelling without siblings or a spouse.  \n",
    "The passengers who were traveling alone or with the family of 3 or more members have a lower survival chance, whereas passengers with 1 or 2 family members and aged between 15 to 25 years had the highest chance of surviving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c722d0ea-2cf8-4f01-ae94-1c2e267b4f99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Subset the data to include only males and females\n",
    "p0_all = df_train[df_train['SibSp'] == 0]['Age']\n",
    "p0_srv = df_train[(df_train['SibSp'] == 0) & (df_train['Survived'] == 1)]['Age']\n",
    "p1_all = df_train[df_train['SibSp'] == 1]['Age']\n",
    "p1_srv = df_train[(df_train['SibSp'] == 1) & (df_train['Survived'] == 1)]['Age']\n",
    "p2_all = df_train[df_train['SibSp'] == 2]['Age']\n",
    "p2_srv = df_train[(df_train['SibSp'] == 2) & (df_train['Survived'] == 1)]['Age']\n",
    "p3_all = df_train[df_train['SibSp'] == 3]['Age']\n",
    "p3_srv = df_train[(df_train['SibSp'] == 3) & (df_train['Survived'] == 1)]['Age']\n",
    "p4_all = df_train[df_train['SibSp'] == 4]['Age']\n",
    "p4_srv = df_train[(df_train['SibSp'] == 4) & (df_train['Survived'] == 1)]['Age']\n",
    "p5_all = df_train[df_train['SibSp'] >= 5]['Age']\n",
    "p5_srv = df_train[(df_train['SibSp'] >= 5) & (df_train['Survived'] == 1)]['Age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98e1101-1192-4d09-a6f7-464de7e6215b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the figure and axis objects\n",
    "fig, ax = plt.subplots(2, 3, figsize=(10, 10))\n",
    "\n",
    "p = [[p0_all, p0_srv], [p1_all, p1_srv], [p2_all, p2_srv], [p3_all, p3_srv], [p4_all, p4_srv], [p5_all, p5_srv]]\n",
    "count=0\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        ax[i][j].hist(p[i+j+count][0], color='red', bins=20, alpha=0.5, label='Total Passengers')\n",
    "        ax[i][j].hist(p[i+j+count][1], color='blue', bins=20, alpha=0.7, label='Survived')\n",
    "        ax[i][j].set_title(f'SibSp = {i+j+count}')\n",
    "        ax[i][j].set_xlabel(\"Age\")\n",
    "        ax[i][j].set_ylabel(\"Count\")\n",
    "        ax[i][j].set_xticks(np.linspace(0, 80, 9))\n",
    "        ax[i][j].set_ylim(0, 150)\n",
    "        ax[i][j].legend(fontsize=8)\n",
    "    count += 2\n",
    "\n",
    "fig.suptitle(\"Survival Likelihood based on Age and Sibling/Spouse count\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd81a53-6cff-41bb-aba5-972760dda4ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b087f4cc-cd4e-45b9-977c-8b33754077ea",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Interaction Feature using `SibSp` and `Parch`\n",
    "\n",
    "This code creates two new features, `FamilySize` and `IsAlone`, from the existing features `Parch` and `SibSp`.\n",
    "These two new features can potentially provide information about the survival of a passenger that is not captured in the `Parch` and `SibSp` features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3781df0-90e7-411b-8218-6c08dafb4d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['FamilySize'] = df_train['Parch'] + df_train['SibSp'] + 1\n",
    "df_train['IsAlone'] = np.where(df_train['FamilySize'] == 1, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271b99f9-b67e-4af5-a2e2-f761f634af35",
   "metadata": {},
   "outputs": [],
   "source": [
    "psgr_srv = df_train[df_train['Survived']==1]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax[0].bar(df_train['FamilySize'].value_counts().index, df_train['FamilySize'].value_counts(), width=0.4, color='red', alpha=0.5, label='Total Passengers')\n",
    "ax[0].bar(psgr_srv['FamilySize'].value_counts().index, psgr_srv['FamilySize'].value_counts(), width=0.4, color='blue', alpha=0.7, label='Passengers Survived')\n",
    "ax[0].set_xticks(np.linspace(0, 11, 12))\n",
    "ax[0].set_xlabel(\"Family Size\")\n",
    "ax[0].set_ylabel(\"Count\")\n",
    "ax[0].set_title(\"Survival Likelihood\\nbased on Family Size\")\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].bar(df_train['IsAlone'].value_counts().index, df_train['IsAlone'].value_counts(), width=0.1, color='red', alpha=0.5, label='Total Passengers')\n",
    "ax[1].bar(psgr_srv['IsAlone'].value_counts().index, psgr_srv['IsAlone'].value_counts(), width=0.1, color='blue', alpha=0.7, label='Passengers Survived')\n",
    "ax[1].set_xticks([0, 1])\n",
    "ax[1].set_xticklabels(['With Family', 'Alone'])\n",
    "ax[1].set_xlabel(\"Alone Traveler?\")\n",
    "ax[1].set_ylabel(\"Count\")\n",
    "ax[1].set_title(\"Survival Likelihood\\nbased on traveling solo/with family\")\n",
    "ax[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ef78f3-8b2b-4067-821d-0e0b70544403",
   "metadata": {},
   "source": [
    "#### Categories from Features \n",
    "\n",
    "##### Age Bins\n",
    "\n",
    "Creating age bins can help in improving accuracy by converting the continuous variable \"age\" into a categorical variable. Converting a continuous variable into a categorical variable can make it easier for the model to detect patterns and relationships between the target variable and the predictor variable. Additionally, age bins can also reduce the impact of noise or outliers in the data and make the variable more interpretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33de67d0-5817-46ad-9079-73b7b08bd4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['AgeBin'] = pd.qcut(df_train['Age'], 8, labels=[1, 2, 3, 4, 5, 6, 7, 8])\n",
    "df_train.groupby(['AgeBin'])['Age'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6327c86e-cfc0-4e78-8567-8f0db30db6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = pd.qcut(df_train['Age'], 8, labels=[1, 2, 3, 4, 5, 6, 7, 8], retbins=True)[1]\n",
    "print(bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca601ef7-5372-4ad0-a4ba-8cb9ef51faf1",
   "metadata": {},
   "source": [
    "##### Fare Bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa45d9a1-5391-4268-9442-39548fb62a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['FarePerPerson'] = df_train['Fare']/(df_train['FamilySize'] + 1)\n",
    "\n",
    "# create fare bins\n",
    "df_train['FareBin'] = pd.qcut(df_train['FarePerPerson'], 5, labels=[1, 2, 3, 4, 5])\n",
    "df_train.groupby(['FareBin'])['FarePerPerson'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dc3025-7e8f-453b-9b2c-5b63251afdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = pd.qcut(df_train['Fare'], 5, labels=[1, 2, 3, 4, 5], retbins=True)[1]\n",
    "print(bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af71c6b5-0436-49fc-804f-81f751bc3b60",
   "metadata": {},
   "source": [
    "##### Family Size Bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c376e27e-208e-4aba-90cb-3c045c136f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['FamilySizeGroup'] = 'Single'\n",
    "df_train.loc[(df_train['FamilySize'] > 1) & (df_train['FamilySize'] <= 4), 'FamilySizeGroup'] = 'Small'\n",
    "df_train.loc[(df_train['FamilySize'] > 4), 'FamilySizeGroup'] = 'Large'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba541b6d-0bc7-4616-b9bf-d65dfd36d5d6",
   "metadata": {},
   "source": [
    "##### Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5aac4b-fd53-48b8-9749-9018276a250d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train['Title'] = df_train.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "df_train['Title'] = df_train['Title'].replace(['Lady', 'Countess','Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "df_train['Title'] = df_train['Title'].replace('Mlle', 'Miss')\n",
    "df_train['Title'] = df_train['Title'].replace('Ms', 'Miss')\n",
    "df_train['Title'] = df_train['Title'].replace('Mme', 'Mrs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8554a6b9-c752-4226-b6e6-b4f086724dc2",
   "metadata": {},
   "source": [
    "##### Creating feature from Ticket Prefix and Ticket Number - Not required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0712cc-ea0f-4e65-82fb-c1d60bfd7f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train['TicketPrefix'] = df_train['Ticket'].apply(lambda x: x.split(' ')[0])\n",
    "#df_train['TicketNumber'] = df_train['Ticket'].apply(lambda x: x.split(' ')[-1])\n",
    "#df_train['TicketCombined'] = df_train['TicketPrefix'] + df_train['TicketNumber']\n",
    "# Apply one-hot encoding\n",
    "#df_train = pd.get_dummies(df_train, columns=['TicketCombined'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddda8437-77d7-44f3-bcdc-4a7fe909891f",
   "metadata": {
    "tags": [
     "featureengg"
    ]
   },
   "outputs": [],
   "source": [
    "# Find columns that are missing in the train dataset\n",
    "# but are available in train dataset. These columns\n",
    "# were added in the train data through feature engineering.\n",
    "#train_cols = set(df_train.columns)\n",
    "#test_cols = set(df_test.columns)\n",
    "#missing_cols = test_cols - train_cols\n",
    "#print(\"No. of Columns missing in train data:\", len(missing_cols))\n",
    "\n",
    "# Add missing columns in the test dataframe\n",
    "#for col in df_test.columns:\n",
    "#    if col.startswith('TicketCombined'):\n",
    "#        if col not in df_train.columns:\n",
    "#            df_train[col] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec626e0-a80a-44ae-903c-e6a37634390b",
   "metadata": {},
   "source": [
    "### Model Development using Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc6ca26-5ae7-4f7b-b5ad-71cdf003e323",
   "metadata": {},
   "source": [
    "#### Create NumPy Arrays from Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7e5ad1-dc10-4637-8ce0-a73e57b53cd3",
   "metadata": {},
   "source": [
    "#### Encode Categorical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40acb911-1cbc-4b81-b0d7-af5893e2659b",
   "metadata": {},
   "source": [
    "`Sex` features in this dataset is categorical. ML libraries does not handle categorical variables and to infer any information from the categorical features such as `Age`, `Titles` etc. needs to be converted to numerical values. We can these features to numerical values using **pandas.get_dummies()** or `LabelEncoder()` method to convert the categorical variable into indicator variables.  \n",
    "\n",
    "For features that contain categories that have numerical relationship, I will use `LabelEncoder` method. As we can see, the categories in feature `FamilySizeGroup` are `Single`, `Small` and `Large`. We can establish a numerical relationship between these categories, thus I will apply the method `LabelEncoder` to `FamilySizeGroup`, which will change the contents of the column to specified numerical values.\n",
    "\n",
    "For features that have independent categories (meaning that there is no relationship between categories such as `male`, `female`), I will use the `pd.get_dummies` method which creates new columns for each category and assign a binary value indicating whether the original value is present or not. These features are `Sex` which has `male` and `female` categories, and `Title` which has `Rare`, `Mr`, `Mrs`, `Miss`, and `Master` categories. This method is called as **One-hot encoding**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abeeae7b-f1fe-4e37-949b-872bda74a9c9",
   "metadata": {},
   "source": [
    "After applying one-hot encoding, I will drop the some columns from our dataframe as these features either (1) don't provide any meaningful information or (2) I have extracted all meaningful information from them by feature engineering and I don't need these features now.  \n",
    "After dropping the columns, convert the Pandas dataframe to a NumPy array. Subsequently, I will apply the LabelEncoder method on the `FamilySizeGroup` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a440f0-b997-431f-b973-76a1707b04f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for Sex and Title features using pd.get_dummies() method\n",
    "encode_sex = pd.get_dummies(df_train['Sex'], prefix='Sex')\n",
    "encode_ttl = pd.get_dummies(df_train['Title'], prefix='Title')\n",
    "df_train = pd.concat([df_train, encode_sex, encode_ttl], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53191b51-cbe0-48f3-9c24-ea57dc798356",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2edbd8c-267a-4de5-b591-4e10af5b34c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns that are not required\n",
    "df = df_train.drop(['Survived', 'Name', 'Fare', 'Sex', 'Ticket', 'Embarked', 'FamilySize', 'FarePerPerson', 'Title'], axis=1)\n",
    "x = df.values\n",
    "df.head()\n",
    "#x[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fb2df4-5355-4690-b42e-5c43fb23d08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb0510a-cbcd-47e7-afe3-eb1e9799d8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode FamilySizeGroup feature using LabelEncoder() method\n",
    "le_fsize = preprocessing.LabelEncoder()\n",
    "le_fsize.fit(['Single','Small', 'Large'])\n",
    "x[:,7] = le_fsize.transform(x[:,7])\n",
    "x[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108df0a9-26f0-49f7-8e22-c34695a9801e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_train['Survived'].values\n",
    "y[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e84e8bf-ccf6-43d2-b5d4-4d542cb230fc",
   "metadata": {},
   "source": [
    "#### Normalize Data\n",
    "\n",
    "Data Standardization gives the data zero mean and unit variance, it is good practice, especially for algorithms such as KNN which is based on the distance of data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d10450d-8f4e-4de4-9bcd-7049f37834cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = preprocessing.StandardScaler().fit(x).transform(x.astype(float))\n",
    "x[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1859fb4f-f3fc-487e-9672-8dc4cac0427a",
   "metadata": {},
   "source": [
    "#### Train-Test Split\n",
    "\n",
    "Although, the test-train data is provided in separate files. I will split the train data into test and train data. I will train the data and then run it on the test data to test the accuracy. I will be training the data several times using different train-to-test ratio. Once a high accuracy is achieved, I will run the code on the actual test data to make the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da32979f-9ee0-4b2d-9392-d4e2d219e704",
   "metadata": {
    "tags": [
     "model"
    ]
   },
   "outputs": [],
   "source": [
    "xtrain_train, xtrain_test, ytrain_train, ytrain_test = train_test_split(x, y, test_size=0.3, random_state=2)\n",
    "print ('Train set:', xtrain_train.shape,  ytrain_train.shape)\n",
    "print ('Test set:', xtrain_test.shape,  ytrain_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4835752b-e349-4347-b27c-5be22a0f3f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y,y_predict):\n",
    "    \"this function plots the confusion matrix\"\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    cm = confusion_matrix(y, y_predict)\n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, ax = ax); #annot=True to annotate cells\n",
    "    ax.set_xlabel('Predicted labels')\n",
    "    ax.set_ylabel('True labels')\n",
    "    ax.set_title('Confusion Matrix'); \n",
    "    ax.xaxis.set_ticklabels(['survived', 'died']); ax.yaxis.set_ticklabels(['survived', 'died'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cd9fcf-dcdb-4470-b39f-ac6623697de9",
   "metadata": {},
   "source": [
    "#### Model - K-Nearest Neighbor (KNN)\n",
    "\n",
    "##### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b0957c-f94c-4cc5-b10d-c6a7a099b7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "              'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute', 'uniform', 'distance'],\n",
    "              'p': [1,2, 3]}\n",
    "\n",
    "print(\"Model - k-Nearest Neighbor\")\n",
    "print(\"==========================\")\n",
    "print(\"Tuning hyperparameters\")\n",
    "\n",
    "\n",
    "KNN = KNeighborsClassifier()\n",
    "knn_cv = GridSearchCV(KNN, param_grid=parameters, cv=20, verbose=0).fit(xtrain_train, ytrain_train)\n",
    "\n",
    "print(\"tuned hpyerparameters :(best parameters) \",knn_cv.best_params_)\n",
    "print(\"accuracy :\",knn_cv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f0ecc8-3af9-426e-9c1c-604366062522",
   "metadata": {},
   "source": [
    "##### Train with best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d36628d-31aa-41ea-92fe-c9d74076b15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=knn_cv.best_params_['n_neighbors'],\n",
    "                           algorithm=knn_cv.best_params_['algorithm'],\n",
    "                           p=knn_cv.best_params_['p']).fit(xtrain_train, ytrain_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eafef2b-2402-4f72-9b71-5f1bd456f486",
   "metadata": {},
   "source": [
    "##### Prediction and Confusion Matrix\n",
    "We can use the model to make predictions on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48454f6c-761f-4a3c-95ef-2359987138f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_score = knn.score(xtrain_test, ytrain_test)\n",
    "print(knn_score)\n",
    "yhat = knn.predict(xtrain_test)\n",
    "plot_confusion_matrix(ytrain_test, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e3b196-5b7b-4203-8409-e0a44b34bcf9",
   "metadata": {},
   "source": [
    "#### Model - Logistic Regression\n",
    "\n",
    "##### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fbd1bc-d126-45e7-8125-b16023b5718f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "              'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "              'fit_intercept': [True, False],\n",
    "              'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n",
    "\n",
    "print(\"Model - Logistic Regression\")\n",
    "print(\"===========================\")\n",
    "print(\"Tuning hyperparameters\")\n",
    "\n",
    "lr = LogisticRegression()\n",
    "logreg_cv = GridSearchCV(lr, param_grid=parameters, cv=20, verbose=0).fit(xtrain_train,ytrain_train)\n",
    "\n",
    "print(\"tuned hpyerparameters :(best parameters) \", logreg_cv.best_params_)\n",
    "print(\"accuracy :\", logreg_cv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7a5767-1e29-4de4-89b9-275e45d4f63a",
   "metadata": {},
   "source": [
    "##### Prediction and Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814becae-2d8d-4632-a1aa-0da380e2fdba",
   "metadata": {},
   "source": [
    "##### Train with best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00eb193c-8123-41d7-a6ac-80f12b0adb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(penalty=logreg_cv.best_params_['penalty'],\n",
    "                            C=logreg_cv.best_params_['C'],\n",
    "                            fit_intercept=logreg_cv.best_params_['fit_intercept'],\n",
    "                            solver=logreg_cv.best_params_['solver']).fit(xtrain_train, ytrain_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fc0971-f8d8-4816-9b37-daf23b2f7dd8",
   "metadata": {
    "tags": [
     "logreg"
    ]
   },
   "outputs": [],
   "source": [
    "logreg_score = logreg.score(xtrain_test, ytrain_test)\n",
    "print(logreg_score)\n",
    "\n",
    "yhat = logreg.predict(xtrain_test)\n",
    "plot_confusion_matrix(ytrain_test, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53e1c11-ffda-4ea9-91fa-14c0649e4132",
   "metadata": {},
   "source": [
    "#### Model - Support Vector Machine\n",
    "\n",
    "##### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef44a537-51f3-4a61-8559-556f5f25a11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'C': [0.1, 1, 10, 100],\n",
    "              'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n",
    "              'degree': [2, 3, 4],\n",
    "              'gamma': [0.1, 0.01, 0.001, 'scale', 'auto']}\n",
    "\n",
    "print(\"Model - Support Vector Maching\")\n",
    "print(\"==============================\")\n",
    "print(\"Tuning hyperparameters\")\n",
    "\n",
    "svm = SVC()\n",
    "svm_cv = GridSearchCV(svm, param_grid=parameters, cv=3, verbose=0).fit(xtrain_train, ytrain_train)\n",
    "\n",
    "print(\"tuned hpyerparameters :(best parameters) \", svm_cv.best_params_)\n",
    "print(\"accuracy :\", svm_cv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11dc97c-675a-464e-bafa-0676556ab4e0",
   "metadata": {},
   "source": [
    "##### Train with best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601b1276-a6cd-4a5e-93ca-bc7dc64dbfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(C=svm_cv.best_params_['C'],\n",
    "          kernel=svm_cv.best_params_['kernel'],\n",
    "          degree=svm_cv.best_params_['degree'],\n",
    "          gamma=svm_cv.best_params_['gamma']).fit(xtrain_train, ytrain_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4e6bd7-3b97-4cae-a246-ac8557fa8b0e",
   "metadata": {},
   "source": [
    "##### Prediction and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e779791a-5588-423f-af9c-bd97223a5f78",
   "metadata": {
    "tags": [
     "svm"
    ]
   },
   "outputs": [],
   "source": [
    "svm_score = svm.score(xtrain_test, ytrain_test)\n",
    "print(svm_score)\n",
    "\n",
    "yhat = svm.predict(xtrain_test)\n",
    "plot_confusion_matrix(ytrain_test, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e89157-e6f4-46c9-823e-47bec9c1f23c",
   "metadata": {},
   "source": [
    "#### Model - Decision Tree Classifier\n",
    "\n",
    "##### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0cfa73-2988-4a5e-b0fb-91306df761c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'criterion': ['gini', 'entropy'],\n",
    "              'splitter': ['best', 'random'],\n",
    "              'max_depth': [2*n for n in range(1,10)],\n",
    "              'max_features': [None, 'auto', 'sqrt'],\n",
    "              'min_samples_leaf': [2, 5, 10, 20, 50],\n",
    "              'min_samples_split': [1, 2, 5, 10, 20]}\n",
    "\n",
    "print(\"Model - Decision Tree\")\n",
    "print(\"=====================\")\n",
    "print(\"Tuning hyperparameters\")\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "tree_cv = GridSearchCV(tree, param_grid=parameters, cv=20, verbose=0).fit(xtrain_train, ytrain_train)\n",
    "\n",
    "print(\"tuned hpyerparameters :(best parameters) \", tree_cv.best_params_)\n",
    "print(\"accuracy :\", tree_cv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaffec6f-ea55-4dc4-8357-f3780c7e13ed",
   "metadata": {},
   "source": [
    "##### Train with best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841ffe86-0428-4c08-b86f-606a46bc47ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(criterion=tree_cv.best_params_['criterion'],\n",
    "                              splitter=tree_cv.best_params_['splitter'],\n",
    "                              max_depth=tree_cv.best_params_['max_depth'],\n",
    "                              max_features=tree_cv.best_params_['max_features'],\n",
    "                              min_samples_leaf=tree_cv.best_params_['min_samples_leaf'],\n",
    "                              min_samples_split=tree_cv.best_params_['min_samples_split']).fit(xtrain_train, ytrain_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe5eff0-0f92-4771-bbe8-87b3c139bed0",
   "metadata": {},
   "source": [
    "##### Prediction and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611c34cd-685c-4c8a-b4b6-7e1e88d5a3be",
   "metadata": {
    "tags": [
     "tree"
    ]
   },
   "outputs": [],
   "source": [
    "tree_score = tree.score(xtrain_test, ytrain_test)\n",
    "print(tree_score)\n",
    "\n",
    "yhat = tree.predict(xtrain_test)\n",
    "plot_confusion_matrix(ytrain_test, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd953e5-c0ad-4335-8a53-3f21c799298a",
   "metadata": {},
   "source": [
    "#### Model - Naive Bayes\n",
    "\n",
    "##### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2165e669-dbb1-4aec-9c57-e8a70f995878",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'var_smoothing': [1e-09, 1e-08, 1e-07, 1e-06, 1e-05]}\n",
    "\n",
    "print(\"Model - Gauss Naive Bayes\")\n",
    "print(\"=========================\")\n",
    "print(\"Tuning var_smoothing\")\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb_cv = GridSearchCV(gnb, param_grid=parameters, cv=20, verbose=0).fit(xtrain_train, ytrain_train)\n",
    "\n",
    "print(\"var_smoothing :(best parameter) \", gnb_cv.best_params_['var_smoothing'])\n",
    "print(\"accuracy :\", gnb_cv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806235a6-2ae7-4676-98cb-1f7c65b16a63",
   "metadata": {},
   "source": [
    "##### Train with best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739851b5-9e10-490f-b799-2becc99aeeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB(var_smoothing=gnb_cv.best_params_['var_smoothing']).fit(xtrain_train, ytrain_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0511d2b-e440-4b2a-8b8a-8733c019d40f",
   "metadata": {},
   "source": [
    "##### Prediciton and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e456aefa-81f9-4fdf-aef7-22ffd7f68fb3",
   "metadata": {
    "tags": [
     "gaussnb"
    ]
   },
   "outputs": [],
   "source": [
    "gnb_score = gnb.score(xtrain_test, ytrain_test)\n",
    "print(gnb_score)\n",
    "\n",
    "yhat = gnb.predict(xtrain_test)\n",
    "plot_confusion_matrix(ytrain_test, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129d7d13-d51f-448f-bfe8-d647661cb9d1",
   "metadata": {},
   "source": [
    "#### Model - XGBoost\n",
    "\n",
    "##### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc57e2d-16ab-4a18-b812-00d27f285ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'learning_rate': [0.1, 0.05, 0.01],\n",
    "              'max_depth': [3, 5, 7],\n",
    "              'min_child_weight': [1, 3, 5],\n",
    "              'n_estimators': [100, 200, 300]\n",
    "              }\n",
    "\n",
    "print(\"Model - XGBoost\")\n",
    "print(\"===============\")\n",
    "print(\"Tuning hyperparameters\")\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "xgb_cv = GridSearchCV(xgb, param_grid=parameters, cv=5, verbose=0).fit(xtrain_train, ytrain_train)\n",
    "\n",
    "\n",
    "print(\"tuned hpyerparameters :(best parameters) \", xgb_cv.best_params_)\n",
    "print(\"accuracy :\", xgb_cv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0466bf-b97e-48f1-9fe9-4c7b3dcdf006",
   "metadata": {},
   "source": [
    "##### Train with best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f77923-e3fd-45c8-8db2-ee253933aa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(learning_rate=xgb_cv.best_params_['learning_rate'],\n",
    "                    max_depth=xgb_cv.best_params_['max_depth'],\n",
    "                    min_child_weight=xgb_cv.best_params_['min_child_weight'],\n",
    "                    n_estimators=xgb_cv.best_params_['n_estimators']).fit(xtrain_train, ytrain_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587a7f2b-4ff9-42d5-abc9-39c05ba189bb",
   "metadata": {},
   "source": [
    "##### Prediciton and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13e28cd-992d-4dea-95b1-02cc472b9d16",
   "metadata": {
    "tags": [
     "xgboost"
    ]
   },
   "outputs": [],
   "source": [
    "xgb_score = xgb.score(xtrain_test, ytrain_test)\n",
    "print(xgb_score)\n",
    "\n",
    "yhat = xgb.predict(xtrain_test)\n",
    "plot_confusion_matrix(ytrain_test, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83f45ea-8e9d-45e2-b6d3-0556efa8929a",
   "metadata": {},
   "source": [
    "#### Model - Random Forest Classifier\n",
    "\n",
    "##### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddf145b-a814-4008-ba38-f707096c727e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters to be tuned\n",
    "parameters = {\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'max_depth': [5, 8, 15, 25, 30],\n",
    "    'min_samples_split': [2, 5, 10, 15, 100],\n",
    "    'min_samples_leaf': [1, 2, 5, 10]\n",
    "}\n",
    "\n",
    "print(\"Model - Random Forest\")\n",
    "print(\"=====================\")\n",
    "print(\"Tuning hyperparameters\")\n",
    "\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "# Use GridSearchCV to perform hyperparameter tuning\n",
    "rfc_cv = GridSearchCV(rfc, param_grid=parameters, cv=5, verbose=0).fit(xtrain_train, ytrain_train)\n",
    "\n",
    "print(\"tuned hpyerparameters :(best parameters) \", rfc_cv.best_params_)\n",
    "print(\"accuracy :\", rfc_cv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b124819e-1c9f-4c08-9dac-b521f219d579",
   "metadata": {},
   "source": [
    "##### Train with best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb5bb29-fb33-402d-9e47-d7875686d6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best parameters to train and test the model\n",
    "rfc = RandomForestClassifier(n_estimators=rfc_cv.best_params_['n_estimators'],\n",
    "                             max_depth=rfc_cv.best_params_['max_depth'],\n",
    "                             min_samples_split=rfc_cv.best_params_['min_samples_split'],\n",
    "                             min_samples_leaf=rfc_cv.best_params_['min_samples_leaf']).fit(xtrain_train, ytrain_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1495323c-cfcd-4227-81f0-d90d5d924b76",
   "metadata": {},
   "source": [
    "##### Prediction and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f841d49-16e8-4bb7-badf-0a79fe18f560",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_score = rfc.score(xtrain_test, ytrain_test)\n",
    "print(rfc_score)\n",
    "\n",
    "yhat = rfc.predict(xtrain_test)\n",
    "plot_confusion_matrix(ytrain_test, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b50ee61-117e-4927-bc5d-1edab4019350",
   "metadata": {},
   "source": [
    "#### Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b41c55c-098f-47f5-9bc0-b5cc589f8cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'KNeighborsClassifier': knn_cv.best_score_,\n",
    "    'DecisionTreeClassifier': tree_cv.best_score_,\n",
    "    'SupportVectorMachine': svm_cv.best_score_,\n",
    "    'LogisticRegression': logreg_cv.best_score_,\n",
    "    'Naive Bayes': gnb_cv.best_score_,\n",
    "    'xGBoost': xgb_cv.best_score_,\n",
    "    'RandomForest': rfc_cv.best_score_\n",
    "}\n",
    "\n",
    "bestmodel = max(models, key=models.get)\n",
    "print(f\"Best performing model is: {bestmodel} with accuracy score: {models.get(bestmodel):<6.3f}\")\n",
    "\n",
    "params = {\n",
    "    'KNeighborsClassifier': knn_cv.best_params_,\n",
    "    'DecisionTreeClassifier': tree_cv.best_params_,\n",
    "    'SupportVectorMachine': svm_cv.best_params_,\n",
    "    'LogisticRegression': logreg_cv.best_params_,\n",
    "    'Naive Bayes': gnb_cv.best_params_,\n",
    "    'xGBoost': xgb_cv.best_params_,\n",
    "    'RandomForest': rfc_cv.best_params_\n",
    "}\n",
    "\n",
    "bestparams = params.get(bestmodel)\n",
    "print(f\"Tuned hyperparameters for the {bestmodel} are:\", bestparams)\n",
    "\n",
    "fig = plt.figure(figsize = (10, 5))\n",
    "bars = plt.bar(*zip(*models.items()), width=0.6)\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.annotate(str(round(height,3)),\n",
    "                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                xytext=(0, 3),  # 3 points vertical offset\n",
    "                textcoords=\"offset points\",\n",
    "                ha='center', va='bottom')\n",
    "plt.ylim(0,1)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43658ba-1266-4cef-8ae5-57bfcebb99a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score = pd.DataFrame({'Models':['KNN', 'Tree', 'SVM', 'LR', 'GaussNB', 'xGBoost', 'RandomFor'],\n",
    "                         'Score':[knn_score, tree_score, svm_score, logreg_score, gnb_score, xgb_score, rfc_score]\n",
    "                         })\n",
    "df_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9408420-1be4-4051-b475-cbaacba81c4f",
   "metadata": {},
   "source": [
    "#### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec9dc3f-5746-4c9b-8f19-64c0f43bfa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from sklearn.model_selection import cross_val_score\n",
    "\n",
    "best_params = tree_cv.best_params_\n",
    "\n",
    "# create an instance of Gaussian Naive Bayes with the best hyperparameters\n",
    "tree = DecisionTreeClassifier(**best_params)\n",
    "\n",
    "# perform cross-validation to evaluate the performance of the model\n",
    "cv_scores = cross_val_score(tree, xtrain_train, ytrain_train, cv=20)\n",
    "\n",
    "# get the mean accuracy from cross-validation\n",
    "mean_accuracy = np.mean(cv_scores)\n",
    "\n",
    "print(\"Mean Accuracy:\", mean_accuracy)\n",
    "print(f\"Mean cross-validation score: {cv_scores.mean():.2f}\")\n",
    "print(f\"Standard deviation: {cv_scores.std():.2f}\")\n",
    "\n",
    "# Select the best model\n",
    "best_model = tree_cv.best_estimator_\n",
    "print(f\"Best model parameters: {tree_cv.best_params_}\")\n",
    "\n",
    "# Re-train the best model on the entire training dataset\n",
    "best_model.fit(xtrain_train, ytrain_train)\n",
    "\n",
    "# Evaluate the final model on a separate test set\n",
    "yhat = best_model.predict(xtrain_test)\n",
    "accuracy = accuracy_score(ytrain_test, yhat)\n",
    "print(f\"Accuracy on test set: {accuracy:.2f}\")\n",
    "\n",
    "# Fine-tune the hyperparameters of the best model using GridSearchCV\n",
    "parameters = {'criterion': ['gini', 'entropy'],\n",
    "     'splitter': ['best', 'random'],\n",
    "     'max_depth': [2*n for n in range(1,10)],\n",
    "     'max_features': ['auto', 'sqrt'],\n",
    "     'min_samples_leaf': [1, 2, 4],\n",
    "     'min_samples_split': [2, 5, 10]} # Define the hyperparameter grid\n",
    "tree_cv = GridSearchCV(best_model, param_grid=parameters, cv=20).fit(xtrain_train, ytrain_train)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52148be4-41b9-4318-bdd7-c6767ed47bbe",
   "metadata": {
    "tags": [
     "crossval"
    ]
   },
   "outputs": [],
   "source": [
    "\"\"\"print(tree_cv.score(xtrain_test, ytrain_test))\n",
    "\n",
    "yhat = tree_cv.predict(xtrain_test)\n",
    "plot_confusion_matrix(ytrain_test, yhat)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d3be84-e4a5-4c45-8e5e-fe80bfe7b714",
   "metadata": {},
   "source": [
    "### Running the Model on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffad0ac-941b-4a2f-9834-5b6a7f712946",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"./kaggle-titanic-comp/test.csv\")\n",
    "df_test.set_index('PassengerId', inplace=True)\n",
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dcd1a0-d4d6-417e-847f-1f3c8c299b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba8819f-865d-4be3-9568-3347ac60e5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b43e7b-0f3b-4d12-9690-38c66e80d9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951b7705-b3d8-4210-982d-2b46348a40da",
   "metadata": {},
   "source": [
    "#### Missing Values in Age Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4fc442-6e89-4538-9f14-34bb125c60db",
   "metadata": {},
   "outputs": [],
   "source": [
    "sex=['male', 'female']\n",
    "pclasses = [1, 2, 3]\n",
    "embarked = ['C', 'Q', 'S']\n",
    "sibsp = [0, 1, 2, 8]\n",
    "parch = [0, 1, 2, 4, 9]\n",
    "\n",
    "median_ages = []\n",
    "for sx in sex:\n",
    "    for pc in pclasses:\n",
    "        for e in embarked:\n",
    "            for s in sibsp:\n",
    "                for p in parch:\n",
    "                    mask = (df_test['Sex'] == sx) & (df_test['Pclass'] == pc) & (df_test['Embarked'] == e) & (df_test['SibSp'] == s) & (df_test['Parch'] == p)\n",
    "                    median_age = df_test.loc[mask, 'Age'].median()\n",
    "                    df_test.loc[mask & (df_test['Age'].isna()), 'Age'] = median_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1552ac4-df6c-4137-a0a7-253812fa07bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_na = df_test[df_test['Age'].isna()]\n",
    "df_na"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2fe72e-49b5-4573-a737-bc0b4c9ec304",
   "metadata": {},
   "source": [
    "**1. Find the median age of passengers traveled in `Pclass=3`, `SibSp=1`, `Parch=0` and leave out the `Sex` and `Embarked` features. I have left out `Sex` feature because I have checked that median is same for both genders when it is included in our calculation criteria.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b56337-4791-403a-a5a4-8227406c6f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "median = df_test[(df_test['Pclass']==3) & (df_test['SibSp']==1) & (df_test['Parch']==0)]['Age'].median()\n",
    "median"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ee5625-d169-490d-bb7f-474b3b7d5f68",
   "metadata": {},
   "source": [
    "Fill the ages of `PassengerId = 1013, 1141, 1165` with the above calculated median age = 25 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c956b62-95d8-472f-98ea-b1d1d5fcaacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pid = [1013, 1141, 1165]\n",
    "\n",
    "for p in pid:\n",
    "    df_test.loc[p, 'Age'] = median"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144e5783-d362-42b0-8afb-3f2b89a7be81",
   "metadata": {},
   "source": [
    "**2. Find the median age of passengers traveled in `Pclass=3`, `SibSp=2`, `Parch=0`, `Sex=male/female` and leave out the `Embarked` feature.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847fae43-c336-4b09-b44e-8d6814e1ce43",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_male = df_test[(df_test['Sex']=='male') & (df_test['Pclass']==3) & (df_test['SibSp']==2) & (df_test['Parch']==0)]['Age'].median()\n",
    "median_female = df_test[(df_test['Sex']=='female') & (df_test['Pclass']==3) & (df_test['SibSp']==2) & (df_test['Parch']==0)]['Age'].median()\n",
    "print(f\"Male median for the above calculation criteria is: {median_male} and female median is: {median_female}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21443626-1773-4903-be95-9bfc57128c0b",
   "metadata": {},
   "source": [
    "Fill the ages of `PassengerId = 921, 1189` with the above calculated male median age = 27 years and `PassengerId = 1019` with female median age = 18 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c4da3c-ebd3-4835-8ff4-312b18ed54e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.loc[921, 'Age'] = median_male\n",
    "df_test.loc[1189, 'Age'] = median_male\n",
    "df_test.loc[1019, 'Age'] = median_female"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8915da-7203-4b44-a7c5-5032b8438491",
   "metadata": {},
   "source": [
    "**3. Find the median age of passengers with the feature `Parch>=4`, `Sex=male/female`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2081fb5-85ca-4b6e-b892-6c1d0d8a43cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_male = df_test[(df_test['Parch']>=4)  & (df_test['Sex']=='male')]['Age'].median()\n",
    "median_female = df_test[(df_test['Parch']>=4)  & (df_test['Sex']=='female')]['Age'].median()\n",
    "print(f\"Male median for the above calculation criteria is: {median_male} and female median is: {median_female}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfa87eb-9f4a-4187-9162-c6ce052576a8",
   "metadata": {},
   "source": [
    "Fill the ages of `PassengerId = 1024, 1257` with the above calculated female median age = 39 yrs and `PassengerId = 1234` with male median age = 40 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb59336-4980-4cd0-b985-32a1a8c039ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.loc[1024, 'Age'] = median_female\n",
    "df_test.loc[1257, 'Age'] = median_female\n",
    "df_test.loc[1234, 'Age'] = median_male"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fea755b-736a-4c35-9a67-b16b2238b1e8",
   "metadata": {},
   "source": [
    "**4. Find the median age of passengers with the feature `Parch=2`, `SibSp=8`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de78d1cc-b0e3-4096-9ede-f1bcf28edf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "median = df_test[(df_test['Parch']==2)  & (df_test['SibSp']==8)]['Age'].median()\n",
    "median"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e780df39-4878-437d-bb84-45c8ae9163de",
   "metadata": {},
   "source": [
    "Fill the age of `PassengerId = 1080` with the above calculated median age = 14.5 yrs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40eb41ab-477f-4c5d-9a5a-e18e9b40328b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.loc[1080, 'Age'] = median"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce86c55-67ed-4b0d-b779-082ea6d77d3b",
   "metadata": {},
   "source": [
    "**5. Find the median age of passengers traveled in `Pclass=3`, `SibSp=0`, `Parch=2`, `Sex=female` and leave out the `Embarked` feature.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757caf55-90b3-4fd6-8c70-86e0e20ff8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "median = df_test[(df_test['Sex']=='female') & (df_test['Pclass']==3) & (df_test['SibSp']==0) & (df_test['Parch']==2)]['Age'].median()\n",
    "median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779eaf89-c3c0-40c3-befb-0d333d76e345",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.loc[1117, 'Age'] = median"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff9426a-bcf0-433f-9fcd-482d383dbb52",
   "metadata": {},
   "source": [
    "**6. Find the median age of passengers traveled in `Pclass=3`, `SibSp=1`, `Parch=2`, `Sex=male` and leave out the `Embarked` feature.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577959d8-a9a4-4ab7-ac75-c890a611b7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "median = df_train[(df_train['Sex']=='male') & (df_train['Pclass']==3) & (df_train['SibSp']==1) & (df_train['Parch']==2)]['Age'].median()\n",
    "median"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae420e8-ab1f-43e8-b317-ce2c45ad678c",
   "metadata": {},
   "source": [
    "Fill the ages of `PassengerId = `1136` with the above calculated median age = 15 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6dcaed-2928-4f27-b243-cc89b844e7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.loc[1136, 'Age'] = median"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730d2a2c-2042-49b8-87ae-3e3afd41af0c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Missing values in Fare column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f99f5d-87a1-4131-bb20-97667c52437a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the details of the row where Fare is missing, i.e. nan\n",
    "df_na = df_test[df_test['Fare'].isna()]\n",
    "df_na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c313e38-a99e-4cdd-baf5-b6604e0b64f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pclass = df_na['Pclass'].values[0]\n",
    "age = df_na['Age'].values[0]\n",
    "sibsp = df_na['SibSp'].values[0]\n",
    "parch = df_na['Parch'].values[0]\n",
    "ticket = df_na['Ticket'].values[0]\n",
    "emb = df_na['Embarked'].values[0]\n",
    "\n",
    "print(pclass, age, sibsp, parch, ticket, emb)\n",
    "\n",
    "# Find the median of rows in the df_test dataframe which meet the conditions similar to the one in the missing fare row\n",
    "msk = (df_test['Pclass'] == pclass) & (df_test['Parch'] == parch) & (df_test['SibSp'] == sibsp) & (df_test['Embarked'] == emb)\n",
    "median_fare = df_test.loc[msk, 'Fare'].median()\n",
    "df_test.loc[1044, 'Fare'] = median_fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6cec11-3592-4e5c-be2d-b3d2c82ed26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b81164-e201-489e-8b14-eb85bcef95b6",
   "metadata": {},
   "source": [
    "#### Feature Engineering on Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44041ba6-87d0-45f8-bdaf-a480f0f2aa64",
   "metadata": {},
   "source": [
    "##### Interaction Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a389a2-487a-4453-9ff3-e74ab062e85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['FamilySize'] = df_test['Parch'] + df_test['SibSp'] + 1\n",
    "df_test['IsAlone'] = np.where(df_test['FamilySize'] == 1, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7487e3-4e2f-4be3-89e5-2e3b8dc8795b",
   "metadata": {},
   "source": [
    "##### Age Bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f4715e-ef5d-4de2-91e2-47f379595781",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['AgeBin'] = pd.qcut(df_test['Age'], 8, labels=[1, 2, 3, 4, 5, 6, 7, 8])\n",
    "df_test.groupby(['AgeBin'])['Age'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1955b560-0747-4db7-ac22-6de896048a26",
   "metadata": {},
   "source": [
    "##### Fare Bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3a8c67-bc15-4c96-b351-fe7b6b197060",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['FarePerPerson'] = df_test['Fare']/(df_test['FamilySize'] + 1)\n",
    "\n",
    "# create fare bins\n",
    "df_test['FareBin'] = pd.qcut(df_test['FarePerPerson'], 5, labels=[1, 2, 3, 4, 5])\n",
    "df_test.groupby(['FareBin'])['FarePerPerson'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca2daaf-d7b1-4a73-b6a8-5466c3965386",
   "metadata": {},
   "source": [
    "##### FamilySize Bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144c21bc-d02b-4873-baa2-57b5486cde75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['FamilySizeGroup'] = 'Single'\n",
    "df_test.loc[(df_test['FamilySize'] > 1) & (df_test['FamilySize'] <= 4), 'FamilySizeGroup'] = 'Small'\n",
    "df_test.loc[(df_test['FamilySize'] > 4), 'FamilySizeGroup'] = 'Large'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48ca60e-1b59-4dfa-bcad-e066b744680e",
   "metadata": {},
   "source": [
    "##### Creating Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a8376f-e84a-48e8-92f8-f32d6b139111",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['Title'] = df_test.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "df_test['Title'] = df_test['Title'].replace(['Lady', 'Countess','Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "df_test['Title'] = df_test['Title'].replace('Mlle', 'Miss')\n",
    "df_test['Title'] = df_test['Title'].replace('Ms', 'Miss')\n",
    "df_test['Title'] = df_test['Title'].replace('Mme', 'Mrs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abdbbaa-f4e3-4366-a673-c632d550398c",
   "metadata": {},
   "source": [
    "##### Creating feature from Ticket Prefix and Ticket Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cf9d25-5244-4e6e-9dd4-5a4592d016bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_test['Ticket'] = df_test['Ticket'].astype(str)\n",
    "#df_test['TicketPrefix'] = df_test['Ticket'].apply(lambda x: x.split(' ')[0])\n",
    "#df_test['TicketNumber'] = df_test['Ticket'].apply(lambda x: x.split(' ')[-1])\n",
    "#df_test['TicketCombined'] = df_test['TicketPrefix'] + df_test['TicketNumber']\n",
    "# Apply one-hot encoding\n",
    "#df_test = pd.get_dummies(df_test, columns=['TicketCombined'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb013ba-cb04-4d80-9576-ea5599c05107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find columns that are missing in test data\n",
    "# but are available in train data. These columns\n",
    "# were added in the train data through feature engineering.\n",
    "#train_cols = set(df_train.columns)\n",
    "#test_cols = set(df_test.columns)\n",
    "#missing_cols = train_cols - test_cols\n",
    "#print(\"No. of Columns missing in test data:\", len(missing_cols))\n",
    "# Add missing columns in the test dataframe\n",
    "#for col in df_train.columns:\n",
    "#    if col.startswith('TicketCombined'):\n",
    "#        if col not in df_test.columns:\n",
    "#            df_test[col] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6721a5d8-9795-47a5-9a91-709bbb86d7d1",
   "metadata": {},
   "source": [
    "#### Create NumPy Arrays from Test Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7e0616-1eb7-4b2b-9c13-2736679ec95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for Sex and Title features using pd.get_dummies() method\n",
    "encode_sex = pd.get_dummies(df_test['Sex'], prefix='Sex')\n",
    "encode_ttl = pd.get_dummies(df_test['Title'], prefix='Title')\n",
    "df_test = pd.concat([df_test, encode_sex, encode_ttl], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cce2cce-d17b-4c54-bb13-b02258518ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb6b3c5-e28f-4647-bf34-32b2b485e20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns that are not required\n",
    "df = df_test.drop(['Name', 'Sex', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'FamilySize', 'FarePerPerson', 'Title'], axis=1)\n",
    "xtest = df.values\n",
    "df.head()\n",
    "#x[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ba12d6-5e00-485d-b7a9-a96cbabb857f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de60f464-0b11-4a52-bbb6-557fbfe73271",
   "metadata": {},
   "source": [
    "#### Encode Categorical Variables in Test Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba8a4d5-1810-44b7-abf7-c673defb76e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode FamilySizeGroup feature using LabelEncoder() method\n",
    "le_fsize = preprocessing.LabelEncoder()\n",
    "le_fsize.fit(['Single','Small', 'Large'])\n",
    "xtest[:,7] = le_fsize.transform(xtest[:,7])\n",
    "xtest[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4403616e-ed80-48d6-bb2a-608b101e1f87",
   "metadata": {},
   "source": [
    "#### Normalize Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de7829c-fefe-40b8-a86f-4f72ac134ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest = preprocessing.StandardScaler().fit(xtest).transform(xtest.astype(float))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61be124-08e7-4652-883e-1a8ea6349fb6",
   "metadata": {},
   "source": [
    "#### Predict Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07d25d9-1a72-4794-a658-01bb4f159463",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = ['KNN', 'LogReg', 'SVM', 'Tree', 'GaussNB', 'XGBoost', 'Forest']\n",
    "\n",
    "models = {'KNN': knn,\n",
    "          'LogReg': logreg,\n",
    "          'SVM': svm,\n",
    "          'Tree': tree,\n",
    "          'GaussNB': gnb,\n",
    "          'XGBoost': xgb,\n",
    "          'Forest': rfc\n",
    "         }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e85a97-5108-46df-959f-d956d0dd0488",
   "metadata": {},
   "source": [
    "#### Save Predictions to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cfa6a3-40b5-4603-948d-3a0697d5b4fa",
   "metadata": {
    "tags": [
     "predict"
    ]
   },
   "outputs": [],
   "source": [
    "for m in model_list:\n",
    "    model = models.get(m)\n",
    "    # Predict\n",
    "    ypred = model.predict(xtest)\n",
    "    # Save predictions to file\n",
    "    file_name = f'./kaggle-titanic-dataset/prediction_{m}.csv'\n",
    "    df_predicted = pd.DataFrame({'Survived':ypred}, index=df_test.index)\n",
    "    df_predicted.to_csv(file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
